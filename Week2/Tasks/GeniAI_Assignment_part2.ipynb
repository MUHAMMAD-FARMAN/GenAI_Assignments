{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrMM2DvFxXCf",
        "outputId": "7aa02986-f8e9-4a1c-b1ca-57c742bbef78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text corpus\n",
        "emma_text = gutenberg.raw('austen-emma.txt')"
      ],
      "metadata": {
        "id": "a-qWIG7Lxv-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(emma_text)\n",
        "print(\"First 5 sentences:\\n\", sentences[:5])\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(emma_text)\n",
        "print(\"\\nFirst 20 words:\\n\", words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh-7VUaGx4fQ",
        "outputId": "f1caee19-8756-4115-a897-6557bd7fc03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences:\n",
            " ['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\", 'Her mother\\nhad died too long ago for her to have more than an indistinct\\nremembrance of her caresses; and her place had been supplied\\nby an excellent woman as governess, who had fallen little short\\nof a mother in affection.', \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\\nless as a governess than a friend, very fond of both daughters,\\nbut particularly of Emma.\", 'Between _them_ it was more the intimacy\\nof sisters.']\n",
            "\n",
            "First 20 words:\n",
            " ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"\\nFirst 20 stemmed words:\\n\", stemmed_words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyCHF1cKyTcu",
        "outputId": "ea1eb97e-27e3-4b32-e13f-1edfa5801eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 20 stemmed words:\n",
            " ['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"\\nFirst 20 lemmatized words:\\n\", lemmatized_words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lksPnfwyg-v",
        "outputId": "70ccf626-e9c2-41fd-a84e-968bbf949c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 20 lemmatized words:\n",
            " ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"\\nFirst 20 words after stop word removal:\\n\", filtered_words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDJn_iEiykpI",
        "outputId": "6d889998-4f95-40a0-b68a-b6f1fbaa7e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 20 words after stop word removal:\n",
            " ['[', 'Emma', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "X8H6kz-H1XSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the text corpus\n",
        "emma_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(emma_text)\n",
        "\n",
        "# Word Tokenization, Lemmatization, and Stop Word Removal\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word.isalnum()]\n",
        "    filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Preprocess each sentence\n",
        "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
        "\n",
        "# Bag-of-Words (BoW) Feature Extraction\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"Bag-of-Words (BoW) Representation:\")\n",
        "print(bow_df.head())\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_sentences)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgiLxzjk1c3a",
        "outputId": "edac50d0-cba6-435d-e6df-7d9976565e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words (BoW) Representation:\n",
            "   1816  23rd  24th  26th  28th  7th  8th  abbey  abbots  abdy  ...  yielding  \\\n",
            "0     1     0     0     0     0    0    0      0       0     0  ...         0   \n",
            "1     0     0     0     0     0    0    0      0       0     0  ...         0   \n",
            "2     0     0     0     0     0    0    0      0       0     0  ...         0   \n",
            "3     0     0     0     0     0    0    0      0       0     0  ...         0   \n",
            "4     0     0     0     0     0    0    0      0       0     0  ...         0   \n",
            "\n",
            "   york  yorkshire  young  younger  youngest  youth  youthful  zeal  zigzag  \n",
            "0     0          0      0        0         0      0         0     0       0  \n",
            "1     0          0      0        0         1      0         0     0       0  \n",
            "2     0          0      0        0         0      0         0     0       0  \n",
            "3     0          0      0        0         0      0         0     0       0  \n",
            "4     0          0      0        0         0      0         0     0       0  \n",
            "\n",
            "[5 rows x 6181 columns]\n",
            "\n",
            "TF-IDF Representation:\n",
            "       1816  23rd  24th  26th  28th  7th  8th  abbey  abbots  abdy  ...  \\\n",
            "0  0.269181   0.0   0.0   0.0   0.0  0.0  0.0    0.0     0.0   0.0  ...   \n",
            "1  0.000000   0.0   0.0   0.0   0.0  0.0  0.0    0.0     0.0   0.0  ...   \n",
            "2  0.000000   0.0   0.0   0.0   0.0  0.0  0.0    0.0     0.0   0.0  ...   \n",
            "3  0.000000   0.0   0.0   0.0   0.0  0.0  0.0    0.0     0.0   0.0  ...   \n",
            "4  0.000000   0.0   0.0   0.0   0.0  0.0  0.0    0.0     0.0   0.0  ...   \n",
            "\n",
            "   yielding  york  yorkshire  young  younger  youngest  youth  youthful  zeal  \\\n",
            "0       0.0   0.0        0.0    0.0      0.0  0.000000    0.0       0.0   0.0   \n",
            "1       0.0   0.0        0.0    0.0      0.0  0.344971    0.0       0.0   0.0   \n",
            "2       0.0   0.0        0.0    0.0      0.0  0.000000    0.0       0.0   0.0   \n",
            "3       0.0   0.0        0.0    0.0      0.0  0.000000    0.0       0.0   0.0   \n",
            "4       0.0   0.0        0.0    0.0      0.0  0.000000    0.0       0.0   0.0   \n",
            "\n",
            "   zigzag  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "\n",
            "[5 rows x 6181 columns]\n"
          ]
        }
      ]
    }
  ]
}